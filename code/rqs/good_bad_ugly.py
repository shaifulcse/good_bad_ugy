# -*- coding: utf-8 -*-
"""good_bad_ugly.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FdkubISuG8CL5mrmCxYdYX0ZqsXMPYgF
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import RandomOverSampler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier

selected_features = ["SLOCStandard",
                     "CommentCodeRation",
                     "Readability",
                     "SimpleReadability",
                     "NVAR",
                     "NCOMP",
                     "McCabe",
                     "IndentSTD",
                     "MaximumBlockDepth",
                     "totalFanOut",
                     "Length",
                     "MaintainabilityIndex",
                     "isPublic",
                     "isStatic",
                     "isGetterSetter",
                     "Parameters",
                     "LocalVariables"
                     ]


def load_data(train_file, test_file):
    train_data = pd.read_csv(train_file, sep='\t')
    test_data = pd.read_csv(test_file, sep='\t')
    return train_data, test_data


def clean_data(data):
    data = data[data.Type != 'noise']
    data = data[data.Type != 'bad']
    X = data[selected_features]
    Y = data["Type"]
    return X, Y


def oversampling(X, Y):
    ros = RandomOverSampler()
    X, Y = ros.fit_resample(X, Y)
    return X, Y


def report_results(type_data, algorithm, actual_y, pred_y):
    print("Results for ", algorithm)
    print("For", type_data)
    accuracy = accuracy_score(actual_y,
                              pred_y)  # https://medium.com/@maxgrossman10/accuracy-recall-precision-f1-score-with-python-4f2ee97e0d6
    print("Accuracy: ", accuracy)

    precision = precision_score(actual_y, pred_y, average='binary', pos_label='ugly')
    print("Precision: ", precision)

    recall = recall_score(actual_y, pred_y, average='binary', pos_label='ugly')
    print("Recall: ", recall)

    # Calculate F1-score
    f1 = f1_score(actual_y, pred_y, average='binary', pos_label='ugly')
    print("F1-Score: ", f1)


def train_model(algorithm, train_x, train_y):
    if algorithm == 'LogisticRegression':
        rf_cls = LogisticRegression(random_state=0, max_iter = 10000, verbose = True)
    if algorithm == "RandomForest":
        rf_cls = RandomForestClassifier(n_estimators=10, max_depth=10, min_samples_split=5,
                                        min_samples_leaf=5, max_features = 8)
    if algorithm == "DecisionTree":
        rf_cls = DecisionTreeClassifier(random_state=0, max_depth=7)

    if algorithm == "AdaBoost":
        rf_cls = AdaBoostClassifier(n_estimators=100, algorithm="SAMME", random_state=0)

    if algorithm == "SVM":
        rf_cls = make_pipeline(StandardScaler(), SVC(max_iter=5000))
    if algorithm == "NN":
        print("In NN")
        rf_cls = MLPClassifier(max_iter= 1000, solver='adam', alpha=0, learning_rate_init = 0.0001,
                            hidden_layer_sizes=(10), warm_start= False, verbose = True)
    rf_cls.fit(train_x, train_y)

    return rf_cls


def predict(X, cls):
    pred_Y = cls.predict(X)
    return pred_Y


if __name__ == "__main__":
    algorithms = ["LogisticRegression", "DecisionTree", "RandomForest", "AdaBoost", "SVM", "NN"]
    train_file = "../../data/ML/train-test/train.csv"
    test_file = "../../data/ML/train-test/test.csv"
    train_data, test_data = load_data(train_file, test_file)

    train_x, train_y = clean_data(train_data)
    train_x, train_y = oversampling(train_x, train_y)

    test_x, test_y = clean_data(test_data)

    algorithm = algorithms[0]
    if algorithm == "NN":
        print ("scaling")
        scaler = StandardScaler()
        # Don't cheat - fit only on training data
        scaler.fit(train_x)
        train_x = scaler.transform(train_x)
        # apply same transformation to test data
        test_x = scaler.transform(test_x)

    cls = train_model(algorithm, train_x, train_y)
    print (cls.coef_[0])
    pred_y = predict(train_x, cls)
    report_results("Training", algorithm, train_y, pred_y)

    pred_y = predict(test_x, cls)
    report_results("Testing", algorithm, test_y, pred_y)

    # tot = 0
    # acc = 0
    # for i in range (pred_y.shape[0]):
    #     print(test_y.values[i], pred_y[i])
    #     if test_y.values[i] == pred_y[i]:
    #         acc += 1
    #     tot += 1
    # print (acc/tot)


